##  疾病百科 crawler

> 爬虫起点：http://jb.9939.com/jbzz_t1/

### 难点
> [疾病详情页](http://jb.9939.com/brk/)

    目标字段有：疾病名称、简介，以及简介下方的表格所有字段。

    其中“简介”，“典型症状”，“临床检查”都有跳转链接，都需要处理。那么问题来了，在scrapy的回调函数处理这个页面时，如何在能够处理这三个链接的同时保证爬虫速度？

    我一开始直截了当地对每个链接用一个`yield Request`进行处理，但把逻辑写完以后发现scrapy是异步执行的，这样处理到时候对每个详情页得到一个item。我想着能不能单独生成一个Request对象，再收集它的返回结果，然而没发现有这样的用法。
    我郁闷了很久后，想到对这些跳转链接用 requests 库来发送 HTTP 请求，而不使用scrapy的内置处理工具。折腾了一翻写完了代码，调试以后发现爬虫速度很慢，而且代理ip经常发生超时。我不清楚为什么在 scrapy 中穿插 requests 会发生经常超时的问题，但很明显，在 scrapy 的一个回调函数里连续使用 requests 库发送了三次请求，先不说网络超时的问题，这样的方式就已经破坏了 scrapy 的异步处理方式，发生了阻塞。
    我还尝试使用多进程和多线程，结合 requests 库来处理这些跳转链接，结果发现还是行不通，速度太慢，而一共有 8000 多个网页，每个网页有三个跳转链接。多进程多线程的代码在`crawl_info`文件夹中。

    最后我回到 scrapy 框架，对回调函数里的每个跳转链接单独生成一个 Request 对象，并传入疾病名称作为标志。在之后的回调函数里，解析完 html 获得结果后，用数据库的更新操作插入新的字段。本项目使用的是 mongodb。这样处理后，爬虫速度果然显著提升，原因就是遵循了 scrapy 的异步处理方式，进程不会发生阻塞。

    捣腾了一天才解决了爬虫速度慢的问题，也是心累。

### 新处理方式&改进
* 建立 ip 代理池后，使用了`scrapy_proxies`开源库来配置代理，避免了自己写下载中间件可能出现的问题。
* 对日志模块的理解和使用更加准确。
* 管道文件用于mongo数据库操作，在之前项目的查询数据库、插入数据库的基础上，加入了一个类方法，用于向mongo已存在的文档中添加新的字段。    